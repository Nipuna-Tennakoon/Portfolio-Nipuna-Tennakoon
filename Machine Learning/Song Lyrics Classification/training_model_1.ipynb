{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explonatory Data Analysis and Understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:41:10.028449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742749870.076873   52449 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742749870.092385   52449 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 22:41:10.202991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length, max\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler,Word2Vec,StringIndexer, Tokenizer,HashingTF,IDF,RegexTokenizer,CountVectorizer,StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import regexp_replace, col, count, when, size\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/23 22:41:19 WARN Utils: Your hostname, nipuna resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/23 22:41:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/23 22:41:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#import dataset to a dataframe in spark\n",
    "dataset=spark.read.csv('Mendeley dataset.csv',header=True,inferSchema=True)\n",
    "dataset=dataset.select('lyrics','genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumn('lyrics', regexp_replace(col('lyrics'), '[^a-zA-Z\\\\s]', ''))\n",
    "dataset = dataset.withColumn(\"lyrics\", regexp_replace(col(\"lyrics\"), r\"\\d+\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              lyrics|genre|\n",
      "+--------------------+-----+\n",
      "|hold time feel br...|  pop|\n",
      "|believe drop rain...|  pop|\n",
      "|sweetheart send l...|  pop|\n",
      "|kiss lips want st...|  pop|\n",
      "|till darling till...|  pop|\n",
      "|convoy light dead...|  pop|\n",
      "|piece mindin worl...|  pop|\n",
      "|care moment hold ...|  pop|\n",
      "|lonely night surr...|  pop|\n",
      "|tear heart seat s...|  pop|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe visualization\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#column names and their properties\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lyric length: 1714\n"
     ]
    }
   ],
   "source": [
    "#check the maximum length of a lyrics\n",
    "max_length = dataset.select(max(length(dataset[\"lyrics\"]))).collect()[0][0]\n",
    "print(\"Max lyric length:\", max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Word2Vec Encoding for Encoding the Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.withColumn(\"lyrics_tokens\", split(dataset[\"lyrics\"], \" \"))\n",
    "# indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "# word2Vec = Word2Vec(vectorSize=300, minCount=1, inputCol=\"lyrics_tokens\", outputCol=\"lyrics_embedding\")\n",
    "# lr = LogisticRegression(featuresCol=\"lyrics_embedding\", labelCol=\"label\")\n",
    "# pipeline = Pipeline(stages=[indexer, word2Vec, lr])\n",
    "# train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"lyrics\", outputCol=\"tokens\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "word2Vec = Word2Vec(vectorSize=300, minCount=1, inputCol=\"filtered_tokens\", outputCol=\"lyrics_embedding\")\n",
    "lr = LogisticRegression(featuresCol=\"lyrics_embedding\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=[indexer,tokenizer,stop_words_remover, word2Vec, lr])\n",
    "train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 22:41:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 231:========================>                                (3 + 4) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.35844296269598125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions on test_data\n",
    "predictions = model.transform(test_data)\n",
    "# Create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy of logistic regression is low. There are two approaches to increase accuracy. Change the encoding method because Word2Vex is static encoding. We can use advanced trained model. Another option is to use advanced model for classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=spark.read.csv('Mendeley dataset.csv',header=True,inferSchema=True)\n",
    "dataset=dataset.select('lyrics','genre')\n",
    "dataset = dataset.withColumn('lyrics', regexp_replace(col('lyrics'), '[^a-zA-Z\\\\s]', ''))\n",
    "dataset = dataset.withColumn(\"lyrics\", regexp_replace(col(\"lyrics\"), r\"\\d+\", ''))\n",
    "train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline stages\n",
    "tokenizer = Tokenizer(inputCol=\"lyrics\", outputCol=\"tokens\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", vocabSize=250, minDF=2)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_vector\")\n",
    "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol=\"tfidf_vector\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, cv, idf, indexer, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1️⃣1️⃣ Train Model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣2️⃣ Make Predictions\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Accuracy: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1️⃣3️⃣ Evaluate Model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvlinux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
